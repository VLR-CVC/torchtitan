Llama3Siglip2Transformer(
  (tok_embeddings): Embedding(128256, 576)
  (layers): ModuleDict(
    (0): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (1): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (2): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (3): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (4): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (5): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (6): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (7): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (8): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (9): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (10): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (11): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (12): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (13): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (14): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (15): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (16): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (17): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (18): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (19): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (20): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (21): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (22): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (23): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (24): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (25): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (26): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (27): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (28): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
    (29): TransformerBlock(
      (attention): Attention(
        (wq): Linear(in_features=576, out_features=576, bias=False)
        (wk): Linear(in_features=576, out_features=192, bias=False)
        (wv): Linear(in_features=576, out_features=192, bias=False)
        (wo): Linear(in_features=576, out_features=576, bias=False)
        (sdpa): ScaledDotProductAttention()
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=576, out_features=2048, bias=False)
        (w2): Linear(in_features=2048, out_features=576, bias=False)
        (w3): Linear(in_features=576, out_features=2048, bias=False)
      )
      (attention_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
      (ffn_norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): RMSNorm((576,), eps=1e-05, elementwise_affine=True)
  (output): Linear(in_features=576, out_features=128256, bias=False)
  (encoder): VisionTransformer(
    (embeddings): VisionEmbeddings(
      (patch_embedding): Linear(in_features=768, out_features=128, bias=True)
      (position_embedding): Embedding(256, 128)
    )
    (layers): ModuleDict(
      (0): TransformerLayer(
        (layer_norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (self_attn): Attention(
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
          (attn): FlexAttention()
        )
        (layer_norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): FeedForward(
          (fc1): Linear(in_features=128, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (1): TransformerLayer(
        (layer_norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (self_attn): Attention(
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
          (attn): FlexAttention()
        )
        (layer_norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): FeedForward(
          (fc1): Linear(in_features=128, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (2): TransformerLayer(
        (layer_norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (self_attn): Attention(
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
          (attn): FlexAttention()
        )
        (layer_norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): FeedForward(
          (fc1): Linear(in_features=128, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (3): TransformerLayer(
        (layer_norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (self_attn): Attention(
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
          (attn): FlexAttention()
        )
        (layer_norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): FeedForward(
          (fc1): Linear(in_features=128, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=128, bias=True)
        )
      )
    )
    (post_layernorm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
  )
  (projector): Projector(
    (w1): Linear(in_features=128, out_features=128, bias=True)
    (w2): Linear(in_features=128, out_features=576, bias=True)
  )
)
